############################    Script Parameters    ############################

## General Params
mode: 'test' # 'train', 'test'
icl: 'summarize' # 'summarize', 'extract', 'qa'
distill: true
max_seq_len: 2048

## Model Params
model_id: 'meta-llama/Meta-Llama-3-8B' # Hugging Face model id
model_path: 'models/Meta-Llama-3-8B'
use_local_model: true
upload_model: false
is_peft: false
is_instruct: false

## Dataset Params
dataset_id: 'gigaword'
train_path: 'data/train_data.json'
test_path: 'data/test_data.json'
use_local_dataset: false
num_workers: 16

############################    Training Parameters    ############################

## Temporary Output Directory for Model Checkpoints
output_dir: ''

## Learning Rate and Scheduler
learning_rate: 0.0002
lr_scheduler_type: 'constant'

## Number of Training Epochs
num_train_epochs: 3

## Batch Sizes
per_device_train_batch_size: 1
per_device_eval_batch_size: 1

## Gradient Accumulation and Checkpointing
gradient_checkpointing: true
gradient_accumulation_steps: 2 # number of steps before performing a backward/update pass

## Optimizer
optim: adamw_torch
weight_decay: 0.01
max_grad_norm: 0.3
warmup_ratio: 0.03

## Logging and Saving
logging_steps: 10 # log every 10 steps
save_strategy: epoch # save checkpoint every epoch
evaluation_strategy: epoch # evaluate every epoch

## Precision
bf16: true
tf32: true

## Miscellaneous
seed: -1 # -1 for random
disable_tqdm: False
load_best_model_at_end: True

## FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp: 'full_shard auto_wrap offload' # remove offload if enough GPU memory
fsdp_config:
    backward_prefetch: 'backward_pre'
    forward_prefetch: 'false'
    use_orig_params: 'false'
